{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal TSMNet demo notebook for inter-session/-subject source-free (SF) offline and online unsupervised domain adaptation (UDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from moabb.datasets import Kalunga2016, MAMEM1, MAMEM2, MAMEM3, Lee2019_SSVEP\n",
    "from moabb.paradigms import SSVEP\n",
    "\n",
    "\n",
    "# from library.utils.torch import StratifiedDomainDataLoader\n",
    "from spdnets.utils.data import StratifiedDomainDataLoader, DomainDataset\n",
    "\n",
    "from spdnets.trainer import Trainer\n",
    "from spdnets.callbacks import EarlyStopping, MomentumBatchNormScheduler\n",
    "from spdnets.model import GyroNet\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm, inv \n",
    "import spdnets.functionals as fn\n",
    "from copy import deepcopy\n",
    "import spdnets.batchnorm as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_align(raw_array):\n",
    "    \"\"\"\n",
    "    Perform Euler alignment on input numpy array ([trials * channels * samples]) \n",
    "    and return the aligned array.\n",
    "    \"\"\"\n",
    "    # Calculate mean covariance matrix\n",
    "    cov_matrices = [np.cov(trial, rowvar=True) for trial in raw_array]\n",
    "    #cov_matrices = np.cov(raw_array,axis=0, rowvar=True)\n",
    "    mean_cov_matrix = np.mean(cov_matrices, axis=0)\n",
    "    \n",
    "    # Compute transformation matrix\n",
    "    trans_matrix = inv(sqrtm(mean_cov_matrix))\n",
    "    \n",
    "    # Apply transformation to all trials using broadcasting\n",
    "    return trans_matrix @ raw_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network and training configuration\n",
    "cfg = dict(\n",
    "    epochs = 100,\n",
    "    batch_size_train = 50,\n",
    "    domains_per_batch = 5,\n",
    "    validation_size = 0.2,\n",
    "    evaluation = 'inter-subject', # or 'inter-subject'\n",
    "    dtype = torch.float64,\n",
    "    training=True, \n",
    "    swd_loss_weight=0,\n",
    "    euler_align=False,\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    mdl_kwargs = dict( \n",
    "    bnorm_dispersion=bn.BatchNormDispersion.SCALAR,\n",
    "    domain_adaptation=True\n",
    ")\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print('GPU')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_eeg_data(X, labels, metadata, original_duration=2, target_duration=1):\n",
    "    n_trials, n_channels, n_samples = X.shape\n",
    "    \n",
    "    # 计算采样率（假设数据是均匀采样的）\n",
    "    sampling_rate = n_samples / original_duration\n",
    "    target_samples = int(target_duration * sampling_rate)\n",
    "    \n",
    "    # 计算每个trial可以拆分成多少个片段\n",
    "    n_segments = int(original_duration / target_duration)\n",
    "    \n",
    "    # 初始化拆分后的数据\n",
    "    X_split = []\n",
    "    labels_split = []\n",
    "    metadata_split = []\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        for j in range(n_segments):\n",
    "            start_idx = j * target_samples\n",
    "            end_idx = start_idx + target_samples\n",
    "            \n",
    "            # 提取数据片段\n",
    "            segment = X[i, :, start_idx:end_idx]\n",
    "            X_split.append(segment)\n",
    "            \n",
    "            # 复制标签（同一个trial的所有片段标签相同）\n",
    "            labels_split.append(labels[i])\n",
    "            \n",
    "            # 复制并修改元数据\n",
    "            meta_row = metadata.iloc[i].copy()\n",
    "            meta_row['original_trial_idx'] = i\n",
    "            meta_row['segment_idx'] = j\n",
    "            meta_row['trial_segment'] = f\"{i}_{j}\"\n",
    "            metadata_split.append(meta_row)\n",
    "    \n",
    "    X_split = np.array(X_split)\n",
    "    labels_split = np.array(labels_split)\n",
    "    metadata_split = pd.DataFrame(metadata_split).reset_index(drop=True)\n",
    "    \n",
    "    return X_split, labels_split, metadata_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfuda_offline(dataset : DomainDataset, model : GyroNet):\n",
    "    model.eval()\n",
    "    model.domainadapt_finetune(dataset.features.to(dtype=cfg['dtype'], device=device), dataset.labels.to(device=device), dataset.domains, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load a MOABB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moabb_ds = Lee2019_SSVEP()\n",
    "n_classes = 4\n",
    "dataset='Lee2019_SSVEP'\n",
    "original_duration =4\n",
    "target_duration = 1\n",
    "moabb_paradigm = SSVEP(n_classes=n_classes, fmin=1, fmax=50, resample=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit and evaluat the model for all domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=42\n",
    "torch.manual_seed(random_seed)\n",
    "model_name = 'GyroNet'\n",
    "for swd_weight in []:\n",
    "    for ea_align in [0]:\n",
    "        for lr in ]:\n",
    "            records = []\n",
    "            records1=[]\n",
    "            if 'inter-session' in cfg['evaluation']:\n",
    "                subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "                groupvarname = 'session'\n",
    "                eval_fashion = 'session'\n",
    "            elif 'inter-subject' in cfg['evaluation']:\n",
    "                subset_iter = iter([None])\n",
    "                groupvarname = 'subject'\n",
    "                eval_fashion = 'subject'\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "            location = f'swd_weight_{swd_weight}_EA_{ea_align}_lr_{lr}'\n",
    "            index = 0\n",
    "            while os.path.exists(f'pretrained/{dataset}/{location}/{index}'):\n",
    "                index += 1\n",
    "            # iterate over subject groups\n",
    "            global_manifold_violation_counters = {}\n",
    "            global_total_forward_calls = 0\n",
    "            \n",
    "            for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "                # get the data from the MOABB paradigm/dataset\n",
    "                X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "                X, labels, metadata = split_eeg_data(X, labels, metadata)\n",
    "\n",
    "\n",
    "                # extract domains = subject/session\n",
    "                metadata['label'] = labels\n",
    "                metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "                domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "                # convert to torch tensors\n",
    "                domain = torch.from_numpy(domain)\n",
    "                if ea_align ==1:\n",
    "                    for i in domain.unique():\n",
    "                        X[domain == i] = euler_align(X[domain == i])\n",
    "                    input_align = 'ea'\n",
    "                else:\n",
    "                    input_align = 'none'\n",
    "                # for du in domain.unique():\n",
    "                #     domain_ixs = domain == du\n",
    "                #     X[domain_ixs] = fn.robust_zscore(X[domain_ixs], per_channel_variance=False, axis=-1)\n",
    "                # X = np.clip(X, -5, 5)\n",
    "                if lr >= 0.01:\n",
    "                    weight_decay = 1e-3\n",
    "                else:\n",
    "                    weight_decay = 1e-4  \n",
    "\n",
    "\n",
    "                X = torch.from_numpy(X)\n",
    "                y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "                y = torch.from_numpy(y)\n",
    "\n",
    "                lab,lab_count = y.unique(return_counts=True)    \n",
    "\n",
    "                # leave one subject or session out\n",
    "                if 'inter-session' in cfg['evaluation']:\n",
    "                    domain_count = len(domain.unique())\n",
    "                elif 'inter-subject' in cfg['evaluation']:\n",
    "                    domain_count = metadata['subject'].nunique()\n",
    "                if domain_count <10:\n",
    "                    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "                else:\n",
    "                    cv_outer = sklearn.model_selection.GroupKFold(n_splits=10)\n",
    "                cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "                # train/validation split stratified across domains and labels\n",
    "                cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "                cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "                # add datadependen model kwargs\n",
    "                mdl_kwargs = deepcopy(cfg['mdl_kwargs'])\n",
    "                mdl_kwargs['num_classes'] = n_classes\n",
    "                mdl_kwargs['num_electrodes'] = X.shape[1]\n",
    "                mdl_kwargs['chunk_size'] = X.shape[2]\n",
    "                mdl_kwargs['domains'] = domain.unique()\n",
    "                # perform outer CV\n",
    "                for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "                \n",
    "                    # split fitting data into train and validation \n",
    "                    cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg['validation_size'])\n",
    "                    train, val = next(cv_inner.split(X[fit], y[fit], cv_inner_group[fit]))\n",
    "\n",
    "                    # adjust number of \n",
    "                    du = domain[fit][train].unique()\n",
    "                    if cfg['domains_per_batch'] > len(du):\n",
    "                        domains_per_batch = len(du)\n",
    "                    else:\n",
    "                        domains_per_batch = cfg['domains_per_batch']\n",
    "\n",
    "                    # split entire dataset into train/validation/test\n",
    "                    ds_train = DomainDataset(X[fit][train], y[fit][train], domain[fit][train])\n",
    "                    ds_val = DomainDataset(X[fit][val], y[fit][val], domain[fit][val])\n",
    "                    \n",
    "\n",
    "                    # create dataloaders\n",
    "                    # for training use specific loader/sampler so taht \n",
    "                    # batches contain a specific number of domains with equal observations per domain\n",
    "                    # and stratified labels\n",
    "                    loader_train = StratifiedDomainDataLoader(ds_train, cfg['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True, drop_last = False)\n",
    "                    #loader_train= torch.torch.utils.data.DataLoader(ds_train,50)\n",
    "                    loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "                    test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "                    # create the model\n",
    "\n",
    "                    model = GyroNet(device=device,dtype=cfg['dtype'],**mdl_kwargs).to(device=device, dtype=cfg['dtype'])\n",
    "                    es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=20, verbose=False)\n",
    "                    \n",
    "                    bn_sched = MomentumBatchNormScheduler(\n",
    "                        epochs=cfg['epochs']-10,\n",
    "                        bs0=cfg['batch_size_train'],\n",
    "                        bs=cfg['batch_size_train']/cfg['domains_per_batch'], \n",
    "                        tau0=0.85\n",
    "                    )\n",
    "\n",
    "                        # create the trainer\n",
    "                    trainer = Trainer(\n",
    "                        max_epochs= cfg['epochs'],\n",
    "                        min_epochs= 70,\n",
    "                        callbacks=[es,bn_sched],\n",
    "                        loss=torch.nn.CrossEntropyLoss(),\n",
    "                        device=device, \n",
    "                        dtype=torch.float64,\n",
    "                        swd_weight=swd_weight,\n",
    "                        lr=lr,\n",
    "                        weight_decay=weight_decay\n",
    "                    )\n",
    "                    # fit the modelzz\n",
    "\n",
    "                    # print parameters\n",
    "                    if cfg['training']:\n",
    "                        save_dir = f'pretrained/{dataset}/{location}/{index}'\n",
    "                        trainer.fit(model, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "                        model.print_manifold_violation_stats()\n",
    "                        os.makedirs(save_dir, exist_ok=True)\n",
    "                        torch.save(model, f'pretrained/{dataset}/{location}/{index}/{ix_subset}_{ix_fold}.pt')\n",
    "                        print(f'ES best epoch={es.best_epoch}')\n",
    "                        res = trainer.test(model, dataloader=loader_train)\n",
    "                        records1.append(dict(input=input_align,latent='gyro',lr=lr,swd_weight=swd_weight,model=model_name,mode='train',dataset=dataset,**res))\n",
    "                        res = trainer.test(model, dataloader=loader_val)\n",
    "                        records1.append(dict(input=input_align,latent='gyro',lr=lr,swd_weight=swd_weight,model=model_name,mode='validation',dataset=dataset,**res))\n",
    "                    else:\n",
    "                        pass\n",
    "                    sfuda_offline_net=torch.load(f'pretrained/{dataset}/{location}/{index}/{ix_subset}_{ix_fold}.pt', map_location=device, weights_only=False)\n",
    "\n",
    "\n",
    "                    global_total_forward_calls += model.total_forward_calls\n",
    "                    for layer_name, violation_count in model.manifold_violation_counters.items():\n",
    "                        if layer_name not in global_manifold_violation_counters:\n",
    "                            global_manifold_violation_counters[layer_name] = 0\n",
    "                        global_manifold_violation_counters[layer_name] += violation_count\n",
    "                    # evaluation\n",
    "\n",
    "                    test_domain=domain[test].unique()\n",
    "                    for test_domain in test_domain:\n",
    "                        if 'inter-session' in cfg['evaluation']:\n",
    "                            subject=ix_subset\n",
    "                        else:\n",
    "                            subject=ix_fold\n",
    "                        print(f\"Subject:{subject}, test domain: {test_domain}\")    \n",
    "                        ds_test = DomainDataset(X[test][domain[test] == test_domain], y[test][domain[test] == test_domain], domain[test][domain[test] == test_domain])\n",
    "                        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "                        sfuda_offline(ds_test, sfuda_offline_net)\n",
    "                        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "                        res2 = res\n",
    "                        print(f\"{model_name},Test results: {res2}\")\n",
    "                        records.append(dict(input=input_align,latent='gyro',lr=lr,swd_weight=swd_weight,model=model_name,dataset=dataset,subject=subject,domain=test_domain, **res))\n",
    "\n",
    "            # save records\n",
    "            resdf = pd.DataFrame(records)\n",
    "            resdf.to_csv(f'pretrained/{dataset}/results_{location}_{index}.csv',index=False)\n",
    "            resdf1 = pd.DataFrame(records1)\n",
    "            resdf1.to_csv(f'pretrained/{dataset}/train_val_results_{location}_{index}.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
